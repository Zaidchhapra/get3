




























































































































































































































































































































































































































































#EDA
import pandas as pd 
import numpy as np 
import seaborn as sns 
from sklearn.datasets import load_wine 
wine = load_wine() 
df = pd.DataFrame(data = wine.data) 
df = pd.read_csv("/content/roller_coasters.csv") 
# https://www.kaggle.com/datasets/thedevastator/statistical-rankings-of-award-winning-roller-coa?resource=download&select=roller_coasters.csv 
df.head() 
df.tail() 
df.shape 
df.info() 
df.sample(n=5)# gives any 5 random rows
df.isnull()
df.isna().any()
df.isnull().sum()
df.nunique()#number of unique entries over columns or rows
df.nsmallest(3,'week')# how the n number of smallest/largest values for specific coulmn
df.nlargest(2,'week')
df.duplicated().sum() 
df.query('name == "ABC"')# filter a dataframe based on a condition
df.dropna()# remove a row or a column from a dataframe which has a None
values = [‘name’] 
print(values) 
df[[“speed”]].boxplot() 
df.length.describe() 
df.length.hist() 
k = df.length.kurt() 
print(k) 
s = df.length.skew() 
print(s) 
sns.pairplot(df) 
sns.scatterplot(x=”length”, y=”park”, data=df) 
corrmat = df.corr() 
print(corrmat) 
sns.heatmap(corrmat, cbar = True, annot = True, square = True)






























































































































#Data Preprocessing
import pandas as pd
import numpy as np
import seaborn as srs
df = pd.read_csv('train.csv') #https://www.kaggle.com/datasets/hesh97/titanicdataset-traincsv
df.info()


cols = ['Name', 'Ticket', 'Cabin']
df = df.drop(cols, axis=1)
df.info()


df1 = df.dropna()
df1.info()


dummies = []
cols = ['Pclass', 'Sex', 'Embarked']
for col in cols:
  dummies.append(pd.get_dummies(df[col])) 
titanic_dummies = pd.concat(dummies, axis=1)
print(titanic_dummies)


df = pd.concat((df,titanic_dummies), axis=1)
df.shape
df = df.drop(['Pclass', 'Sex', 'Embarked'], axis=1)
df.info()


df['Age'] = df['Age'].interpolate()
df.info()


X = df.values
y = df['Survived'].values
X = np.delete(X, 1, axis=1)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3 , random_state=0)















































































































































































































#LINEARREGRESSION
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
X, y = datasets.load_diabetes(return_X_y=True)
X = X[:20,[2]]
y = y[:20]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
print(X_train[:5,:]) # Show first five data points
regr = LinearRegression(fit_intercept=True)
regr.fit(X_train,y_train)

print('Coefficients', regr.coef_)
print('Intercept', regr.intercept_)

y_pred_test=regr.predict(X_test)
print("Score: %.2f"%regr.score(X_test,y_pred_test))

# Commented out IPython magic to ensure Python compatibility.
# The mean squared error
print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred_test))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred_test))

plt.scatter(X_train, y_train, color='black', label='Train data points')
plt.scatter(X_test, y_test, color='red', label='Test data points')
plt.plot(X_test, y_pred_test, color='blue', linewidth=1, label='Model')
plt.scatter(X_test, y_pred_test, marker='x', color='red', linewidth=3, label='Test Pred')
plt.legend()
plt.show()

































































































































































































#KNN
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import seaborn as sns
sns.set()
breast_cancer = load_breast_cancer()
X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)
X = X[['mean area', 'mean compactness']]
y = pd.Categorical.from_codes(breast_cancer.target,breast_cancer.target_names)
y = pd.get_dummies(y, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

sns.scatterplot( 
x='mean area', y='mean compactness', 
hue='benign', 
data=X_test.join(y_test, how='outer') )

plt.scatter( 
X_test['mean area'], 
X_test['mean compactness'], 
c=y_pred, 
cmap='coolwarm', 
alpha=0.7
) 

confusion_matrix(y_test, y_pred) 
accuracy_score(y_test, y_pred)









































































































































































#WEBSCRAPPING

import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import numpy as np
url = "https://realpython.github.io/fake-jobs/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
print(soup)

results = soup.find(id="ResultsContainer")
print(results.prettify())

job_elements = results.find_all("div", class_="card-content")
df = pd.DataFrame({"title":[],"company":[],"location":[]})

i=0
for job_element in job_elements:
  title = job_element.find("h2", class_="title").text.strip()
  company = job_element.find("h3", class_="company").text.strip()
  location = job_element.find("p", class_="location").text.strip()
  df.loc[i] = [title,company,location]
  i+=1
df.head()






















































































































































































































#Descition tree

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, r2_score
df = pd.read_csv('/content/car_evaluation.csv',header=None,names=['buying', 'maint', 'doors', 'persons', 'boot_space', 'safety', 'output']) #https://www.kaggle.com/datasets/elikplim/car-evaluation-data-set
df.head()


print("Shape \n", df.shape)
print("Info\n",df.info)
print("class val count \n", df['output'].value_counts())


df.isnull().sum()
x = df.drop(['output'], axis=1)
y = df['output']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)


!pip install category_encoders
import category_encoders as ce
encoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'boot_space', 'safety'])
x_train = encoder.fit_transform(x_train)
x_test = encoder.transform(x_test)
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
y_pred = dt.predict(x_test)
print('accuracy score', accuracy_score(y_test, y_pred))


plt.figure(figsize=(12,8))
tree.plot_tree(dt.fit(x_train, y_train))

import graphviz 
dot_data = tree.export_graphviz(dt, out_file=None, 
                              feature_names=x_train.columns,  
                              class_names=y_train,  
                              filled=True, rounded=True,  
                              special_characters=True)
graph = graphviz.Source(dot_data) 
graph
















































































































































































#k-means clustering

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
iris = pd.read_csv("Iris.csv")#https://www.kaggle.com/datasets/saurabh00007/iriscsv
x = iris.iloc[:, [0, 1, 2, 3]].values


iris_setosa=iris.loc[iris["Species"]=="Iris-setosa"]
iris_virginica=iris.loc[iris["Species"]=="Iris-virginica"]
iris_versicolor=iris.loc[iris["Species"]=="Iris-versicolor"]


#Finding the optimum number of clusters for k-means classification
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') #within cluster sum of squares
plt.show()


kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)


#Visualising the clusters
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')


#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')
plt.legend()















































































